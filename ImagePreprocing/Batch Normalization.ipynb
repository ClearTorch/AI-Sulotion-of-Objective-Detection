{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 批量归一化\n",
    "批量归一化（batch normalization）层，它能让较深的神经网络的训练变得更加容易，标准化处理输入数据使各个特征的分布相近：这往往更容易训练出有效的模型。\n",
    "\n",
    "通常来说，数据标准化预处理对于浅层模型就足够有效了。随着模型训练的进行，当每层中参数更新时，靠近输出层的输出较难出现剧烈变化。但对深层神经网络来说，即使输入数据已做标准化，训练中模型参数的更新依然很容易造成靠近输出层输出的剧烈变化。这种计算数值的不稳定性通常令我们难以训练出有效的深度模型。\n",
    "\n",
    "批量归一化的提出正是为了应对深度模型训练的挑战。在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。\n",
    "## 批量归一化层\n",
    "对全连接层和卷积层做批量归一化的方法稍有不同。\n",
    "### 对全连接层做批量归一化\n",
    "对全连接层做批量归一化。通常，将批量归一化层置于全连接层中的仿射变换和激活函数之间。设全连接层的输入为$\\boldsymbol{u}$，权重参数和偏差参数分别为$\\boldsymbol{W}$和$\\boldsymbol{b}$，激活函数为$\\phi$。设批量归一化的运算符为$\\text{BN}$。那么，使用批量归一化的全连接层的输出为\n",
    "\n",
    "$$\\phi(\\text{BN}(\\boldsymbol{x})),$$\n",
    "\n",
    "其中批量归一化输入$\\boldsymbol{x}$由仿射变换\n",
    "\n",
    "$$\\boldsymbol{x} = \\boldsymbol{W\\boldsymbol{u} + \\boldsymbol{b}}$$\n",
    "\n",
    "得到。考虑一个由$m$个样本组成的小批量，仿射变换的输出为一个新的小批量$\\mathcal{B} = {\\boldsymbol{x}^{(1)}, \\ldots, \\boldsymbol{x}^{(m)} }$。它们正是批量归一化层的输入。对于小批量$\\mathcal{B}$中任意样本$\\boldsymbol{x}^{(i)} \\in \\mathbb{R}^d, 1 \\leq i \\leq m$，批量归一化层的输出同样是$d$维向量\n",
    "\n",
    "$$\\boldsymbol{y}^{(i)} = \\text{BN}(\\boldsymbol{x}^{(i)}),$$\n",
    "\n",
    "并由以下几步求得。首先，对小批量$\\mathcal{B}$求均值和方差：\n",
    "\n",
    "$$\\boldsymbol{\\mu}\\mathcal{B} \\leftarrow \\frac{1}{m}\\sum{i = 1}^{m} \\boldsymbol{x}^{(i)},$$ $$\\boldsymbol{\\sigma}\\mathcal{B}^2 \\leftarrow \\frac{1}{m} \\sum{i=1}^{m}(\\boldsymbol{x}^{(i)} - \\boldsymbol{\\mu}_\\mathcal{B})^2,$$\n",
    "\n",
    "其中的平方计算是按元素求平方。接下来，使用按元素开方和按元素除法对$\\boldsymbol{x}^{(i)}$标准化：\n",
    "\n",
    "$$\\hat{\\boldsymbol{x}}^{(i)} \\leftarrow \\frac{\\boldsymbol{x}^{(i)} - \\boldsymbol{\\mu}\\mathcal{B}}{\\sqrt{\\boldsymbol{\\sigma}\\mathcal{B}^2 + \\epsilon}},$$\n",
    "\n",
    "这里$\\epsilon > 0$是一个很小的常数，保证分母大于0。在上面标准化的基础上，批量归一化层引入了两个可以学习的模型参数，拉伸（scale）参数 $\\boldsymbol{\\gamma}$ 和偏移（shift）参数 $\\boldsymbol{\\beta}$。这两个参数和$\\boldsymbol{x}^{(i)}$形状相同，皆为$d$维向量。它们与$\\hat{\\boldsymbol{x}}^{(i)}$分别做按元素乘法（符号$\\odot$）和加法计算：\n",
    "\n",
    "$${\\boldsymbol{y}}^{(i)} \\leftarrow \\boldsymbol{\\gamma} \\odot \\hat{\\boldsymbol{x}}^{(i)} + \\boldsymbol{\\beta}.$$\n",
    "\n",
    "至此，得到了$\\boldsymbol{x}^{(i)}$的批量归一化的输出$\\boldsymbol{y}^{(i)}$。 值得注意的是，可学习的拉伸和偏移参数保留了不对$\\boldsymbol{x}^{(i)}$做批量归一化的可能：此时只需学出$\\boldsymbol{\\gamma} = \\sqrt{\\boldsymbol{\\sigma}\\mathcal{B}^2 + \\epsilon}$和$\\boldsymbol{\\beta} = \\boldsymbol{\\mu}\\mathcal{B}$。可以对此这样理解：如果批量归一化无益，理论上，学出的模型可以不使用批量归一化。\n",
    "\n",
    "### 对卷积层做批量归一化\n",
    "对卷积层来说，批量归一化发生在卷积计算之后、应用激活函数之前。如果卷积计算输出多个通道，我们需要对这些通道的输出分别做批量归一化，且每个通道都拥有独立的拉伸和偏移参数，并均为标量。设小批量中有$m$个样本。在单个通道上，假设卷积计算输出的高和宽分别为$p$和$q$。我们需要对该通道中$m \\times p \\times q$个元素同时做批量归一化。对这些元素做标准化计算时，我们使用相同的均值和方差，即该通道中$m \\times p \\times q$个元素的均值和方差。\n",
    "\n",
    "### 预测时的批量归一化\n",
    "使用批量归一化训练时，可以将批量大小设得大一点，从而使批量内样本的均值和方差的计算都较为准确。将训练好的模型用于预测时，我们希望模型对于任意输入都有确定的输出。因此，单个样本的输出不应取决于批量归一化所需要的随机小批量中的均值和方差。一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。可见，和丢弃层一样，批量归一化层在训练模式和预测模式下的计算结果也是不一样的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet 使用批量归一化\n",
    "tf.keras中layers模块定义的BatchNorm类使用起来更加简单。\n",
    "\n",
    "tf.keras.layers.BatchNormalization(\n",
    "    axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,\n",
    "    beta_initializer='zeros', gamma_initializer='ones',\n",
    "    moving_mean_initializer='zeros', moving_variance_initializer='ones',\n",
    "    beta_regularizer=None, gamma_regularizer=None, beta_constraint=None,\n",
    "    gamma_constraint=None, renorm=False, renorm_clipping=None, renorm_momentum=0.99,\n",
    "    fused=None, trainable=True, virtual_batch_size=None, adjustment=None, name=None,\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "Normalize the activations of the previous layer at each batch, i.e. applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1.\n",
    "\n",
    "Batch normalization differs from other layers in several key aspects:\n",
    "\n",
    "1) Adding BatchNormalization with training=True to a model causes the result of one example to depend on the contents of all other examples in a minibatch. Be careful when padding batches or masking examples, as these can change the minibatch statistics and affect other examples.\n",
    "\n",
    "2) Updates to the weights (moving statistics) are based on the forward pass of a model rather than the result of gradient computations.\n",
    "\n",
    "3) When performing inference using a model containing batch normalization, it is generally (though not always) desirable to use accumulated statistics rather than mini-batch statistics. This is accomplished by passing training=False when calling the model, or using model.predict.\n",
    "\n",
    "Arguments:\n",
    "* axis: Integer, the axis that should be normalized (typically the features axis). For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
    "* momentum: Momentum for the moving average.\n",
    "* epsilon: Small float added to variance to avoid dividing by zero.\n",
    "* center: If True, add offset of beta to normalized tensor. If False, beta is ignored.\n",
    "* scale: If True, multiply by gamma. If False, gamma is not used. When the next layer is linear (also e.g. nn.relu), this can be disabled since the scaling will be done by the next layer.\n",
    "* beta_initializer: Initializer for the beta weight.\n",
    "* gamma_initializer: Initializer for the gamma weight.\n",
    "* moving_mean_initializer: Initializer for the moving mean.\n",
    "* moving_variance_initializer: Initializer for the moving variance.\n",
    "* beta_regularizer: Optional regularizer for the beta weight.\n",
    "* gamma_regularizer: Optional regularizer for the gamma weight.\n",
    "* beta_constraint: Optional constraint for the beta weight.\n",
    "* gamma_constraint: Optional constraint for the gamma weight.\n",
    "* renorm: Whether to use Batch Renormalization (https://arxiv.org/abs/1702.03275). This adds extra variables during training. The inference is the same for either value of this parameter.\n",
    "renorm_clipping: A dictionary that may map keys 'rmax', 'rmin', 'dmax' to scalar Tensors used to clip the renorm correction. The correction (r, d) is used as corrected_value = normalized_value * r + d, with r clipped to [rmin, rmax], and d to [-dmax, dmax]. Missing rmax, rmin, dmax are set to inf, 0, inf, respectively.\n",
    "renorm_momentum: Momentum used to update the moving means and standard deviations with renorm. Unlike momentum, this affects training and should be neither too small (which would add noise) nor too large (which would give stale estimates). Note that momentum is still applied to get the means and variances for inference.\n",
    "* fused: if True, use a faster, fused implementation, or raise a ValueError if the fused implementation cannot be used. If None, use the faster implementation if possible. If False, do not used the fused implementation.\n",
    "* trainable: Boolean, if True the variables will be marked as trainable.\n",
    "* virtual_batch_size: An int. By default, virtual_batch_size is None, which means batch normalization is performed across the whole batch. When virtual_batch_size is not None, instead perform \"Ghost Batch Normalization\", which creates virtual sub-batches which are each normalized separately (with shared gamma, beta, and moving statistics). Must divide the actual batch size during execution.\n",
    "* adjustment: A function taking the Tensor containing the (dynamic) shape of the input tensor and returning a pair (scale, bias) to apply to the normalized values (before gamma and beta), only during training. For example, if axis==-1, adjustment = lambda shape: ( tf.random.uniform(shape[-1:], 0.93, 1.07), tf.random.uniform(shape[-1:], -0.1, 0.1)) will scale the normalized value by up to 7% up or down, then shift the result by up to 0.1 (with independent scaling and bias for each feature but shared across all examples), and finally apply gamma and/or beta. If None, no adjustment is applied. Cannot be specified if virtual_batch_size is specified.\n",
    "\n",
    "Call arguments:\n",
    "* inputs: Input tensor (of any rank).\n",
    "* training: Python boolean indicating whether the layer should behave in training mode or in inference mode.\n",
    "* training=True: The layer will normalize its inputs using the mean and variance of the current batch of inputs.\n",
    "* training=False: The layer will normalize its inputs using the mean and variance of its moving statistics, learned during training.\n",
    "Input shape:\n",
    "\n",
    "Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model.\n",
    "\n",
    "Output shape:\n",
    "\n",
    "Same shape as input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T02:16:34.802035Z",
     "start_time": "2020-06-22T02:16:33.134304Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.2.0\n",
      "Numpy version: 1.18.1\n",
      "Pandas version: 1.0.1\n",
      "Plotly version: 4.8.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, losses\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import plotly as py \n",
    "import plotly.graph_objects as go \n",
    "import datetime\n",
    "\n",
    "print('Tensorflow version:', tf.__version__)\n",
    "print('Numpy version:', np.__version__)\n",
    "print('Pandas version:', pd.__version__)\n",
    "print('Plotly version:', py.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T02:16:39.370621Z",
     "start_time": "2020-06-22T02:16:39.350705Z"
    }
   },
   "outputs": [],
   "source": [
    "def bulid_lenet_batchnorm():\n",
    "    net = models.Sequential()\n",
    "    net.add(layers.Conv2D(filters=6,kernel_size=5, input_shape=(28,28,1)))\n",
    "    net.add(layers.BatchNormalization())\n",
    "    net.add(layers.Activation('sigmoid'))\n",
    "    net.add(layers.MaxPool2D(pool_size=2, strides=2))\n",
    "    net.add(layers.Conv2D(filters=16,kernel_size=5))\n",
    "    net.add(layers.BatchNormalization())\n",
    "    net.add(layers.Activation('sigmoid'))\n",
    "    net.add(layers.MaxPool2D(pool_size=2, strides=2))\n",
    "    net.add(layers.Flatten())\n",
    "    net.add(layers.Dense(120))\n",
    "    net.add(layers.BatchNormalization())\n",
    "    net.add(layers.Activation('sigmoid'))\n",
    "    net.add(layers.Dense(84))\n",
    "    net.add(layers.BatchNormalization())\n",
    "    net.add(layers.Activation('sigmoid'))\n",
    "    net.add(layers.Dense(10,activation='sigmoid'))\n",
    "    \n",
    "    net.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "    net.summary()\n",
    "    return net\n",
    "\n",
    "def train_lenet(net, batch_size, epochs):\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "    x_train = x_train.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "    x_test = x_test.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "    %load_ext tensorboard\n",
    "    log_dir = './log/lenetnorm/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir, histogram_freq=1)\n",
    "    history = net.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2, callbacks=[tensorboard_callback])\n",
    "    net.save_weights('./ModelTrained/lenetnorm_weights')\n",
    "    %tensorboard --logdir log/lenetnorm\n",
    "    test_scores = net.evaluate(x_test, y_test, verbose=2)\n",
    "    print('Test loss:', test_scores[0])\n",
    "    print('Test accuracy:', test_scores[1])\n",
    "    return net\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T02:17:09.200045Z",
     "start_time": "2020-06-22T02:16:48.556222Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 24, 24, 6)         156       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 24, 24, 6)         24        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 24, 24, 6)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 8, 8, 16)          2416      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 8, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               30840     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 120)               480       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 84)                336       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 84)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 45,330\n",
      "Trainable params: 44,878\n",
      "Non-trainable params: 452\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "  2/750 [..............................] - ETA: 1:42 - loss: 2.3190 - accuracy: 0.0859WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.141706). Check your callbacks.\n",
      "750/750 [==============================] - 4s 5ms/step - loss: 0.4191 - accuracy: 0.9348 - val_loss: 0.1648 - val_accuracy: 0.9572\n",
      "Epoch 2/5\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 0.0876 - accuracy: 0.9760 - val_loss: 0.1323 - val_accuracy: 0.9600\n",
      "Epoch 3/5\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 0.0620 - accuracy: 0.9815 - val_loss: 0.2290 - val_accuracy: 0.9271\n",
      "Epoch 4/5\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 0.0516 - accuracy: 0.9839 - val_loss: 0.1645 - val_accuracy: 0.9459\n",
      "Epoch 5/5\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 0.0429 - accuracy: 0.9871 - val_loss: 0.1534 - val_accuracy: 0.9540\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 9100), started 3 days, 0:01:07 ago. (Use '!kill 9100' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1817843c46eb0f9a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1817843c46eb0f9a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 0.1423 - accuracy: 0.9538\n",
      "Test loss: 0.14229196310043335\n",
      "Test accuracy: 0.9538000226020813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x1b1a9e7bd08>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lenet(bulid_lenet_batchnorm(),64, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-22T02:18:06.029Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir log/lenetnorm port = 6006"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
